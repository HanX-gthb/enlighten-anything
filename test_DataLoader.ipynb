{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901b3d51-d5da-453c-8150-79137cc22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import random\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from multi_read_data import MemoryFriendlyLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8f2ec7-85e2-4640-9dd7-8763d08ef10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 10 14:18:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:4F:00.0 Off |                  N/A |\n",
      "| 46%   23C    P8    29W / 350W |   2195MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66db103b-40a9-41f7-a30c-eb847ccd2c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../LOL/our485/high'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3612/3208675269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlow_img_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msem_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_img_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LOL/our485/high'"
     ]
    }
   ],
   "source": [
    "low_img_dir = '../LOL/our485/high'\n",
    "sem_dir = \"../LOL/our485/high_semantic\"\n",
    "\n",
    "low_img_names = []\n",
    "sem_names = []\n",
    "for name in os.listdir(low_img_dir):\n",
    "    if name.endswith('.jpg') or name.endswith('.png'):\n",
    "        \n",
    "        # print(root)\n",
    "        # break\n",
    "        low_img_names.append(os.path.join(root, name))\n",
    "        sem_names.append(os.path.join(root, f'{os.path.splitext(name)[0]}_semantic.png'))\n",
    "        \n",
    "\n",
    "\n",
    "# for root, dirs, names in os.walk(sem_dir):\n",
    "#     for name in names:\n",
    "#         if name.endswith('.jpg') or name.endswith('.png'):\n",
    "#             sem_names.append(os.path.join(root, name))\n",
    "\n",
    "\n",
    "# low_img_names.sort()\n",
    "# sem_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa950c78-337d-4fff-a99a-c2635c0d977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../LOL/our485/high/638.png',\n",
       " '../LOL/our485/high/176.png',\n",
       " '../LOL/our485/high/88.png',\n",
       " '../LOL/our485/high/610.png',\n",
       " '../LOL/our485/high/604.png',\n",
       " '../LOL/our485/high/162.png',\n",
       " '../LOL/our485/high/189.png',\n",
       " '../LOL/our485/high/77.png',\n",
       " '../LOL/our485/high/63.png',\n",
       " '../LOL/our485/high/758.png']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_img_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afd59a49-689b-416d-b2bf-7fd45b741c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../LOL/our485/high/638_semantic.png',\n",
       " '../LOL/our485/high/176_semantic.png',\n",
       " '../LOL/our485/high/88_semantic.png',\n",
       " '../LOL/our485/high/610_semantic.png',\n",
       " '../LOL/our485/high/604_semantic.png',\n",
       " '../LOL/our485/high/162_semantic.png',\n",
       " '../LOL/our485/high/189_semantic.png',\n",
       " '../LOL/our485/high/77_semantic.png',\n",
       " '../LOL/our485/high/63_semantic.png',\n",
       " '../LOL/our485/high/758_semantic.png']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9571071a-f4d4-4712-8078-13a13b742fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_data_names = '../LOL/train480/high'\n",
    "sem_dir = \"../LOL/train480/high_semantic\"\n",
    "TrainDataset = MemoryFriendlyLoader(train_low_data_names, sem_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca386b7-30c2-4f14-9c55-deb017f61ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queue = torch.utils.data.DataLoader(\n",
    "    TrainDataset, batch_size=5,\n",
    "    pin_memory=True, \n",
    "    # num_workers=0, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c2b7dd-99dd-4194-832a-411fce531123",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
       "the given dataset.\n",
       "\n",
       "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
       "iterable-style datasets with single- or multi-process loading, customizing\n",
       "loading order and optional automatic batching (collation) and memory pinning.\n",
       "\n",
       "See :py:mod:`torch.utils.data` documentation page for more details.\n",
       "\n",
       "Args:\n",
       "    dataset (Dataset): dataset from which to load the data.\n",
       "    batch_size (int, optional): how many samples per batch to load\n",
       "        (default: ``1``).\n",
       "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
       "        at every epoch (default: ``False``).\n",
       "    sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
       "        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
       "        implemented. If specified, :attr:`shuffle` must not be specified.\n",
       "    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
       "        returns a batch of indices at a time. Mutually exclusive with\n",
       "        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
       "        and :attr:`drop_last`.\n",
       "    num_workers (int, optional): how many subprocesses to use for data\n",
       "        loading. ``0`` means that the data will be loaded in the main process.\n",
       "        (default: ``0``)\n",
       "    collate_fn (callable, optional): merges a list of samples to form a\n",
       "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
       "        map-style dataset.\n",
       "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
       "        into CUDA pinned memory before returning them.  If your data elements\n",
       "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
       "        see the example below.\n",
       "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
       "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
       "        the size of dataset is not divisible by the batch size, then the last batch\n",
       "        will be smaller. (default: ``False``)\n",
       "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
       "        from workers. Should always be non-negative. (default: ``0``)\n",
       "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
       "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
       "        input, after seeding and before data loading. (default: ``None``)\n",
       "    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
       "        by RandomSampler to generate random indexes and multiprocessing to generate\n",
       "        `base_seed` for workers. (default: ``None``)\n",
       "    prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
       "        in advance by each worker. ``2`` means there will be a total of\n",
       "        2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
       "    persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
       "        the worker processes after a dataset has been consumed once. This allows to\n",
       "        maintain the workers `Dataset` instances alive. (default: ``False``)\n",
       "\n",
       "\n",
       ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
       "             cannot be an unpicklable object, e.g., a lambda function. See\n",
       "             :ref:`multiprocessing-best-practices` on more details related\n",
       "             to multiprocessing in PyTorch.\n",
       "\n",
       ".. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
       "             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
       "             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
       "             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
       "             configurations. This represents the best guess PyTorch can make because PyTorch\n",
       "             trusts user :attr:`dataset` code in correctly handling multi-process\n",
       "             loading to avoid duplicate data.\n",
       "\n",
       "             However, if sharding results in multiple workers having incomplete last batches,\n",
       "             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
       "             be broken into multiple ones and (2) more than one batch worth of samples can be\n",
       "             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
       "             cases in general.\n",
       "\n",
       "             See `Dataset Types`_ for more details on these two types of datasets and how\n",
       "             :class:`~torch.utils.data.IterableDataset` interacts with\n",
       "             `Multi-process data loading`_.\n",
       "\n",
       ".. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
       "             :ref:`data-loading-randomness` notes for random seed related questions.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f681e6-cb62-4e3f-b5f3-4d41f939db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 10 13:48:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:4F:00.0 Off |                  N/A |\n",
      "| 45%   23C    P8    29W / 350W |   2195MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d392d3-bca7-4ab7-9656-c7c888f2f384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda torch.Size([5, 3, 400, 600])\n",
      "cuda torch.Size([5, 3, 400, 600])\n",
      "cuda torch.Size([5, 3, 400, 600])\n",
      "cuda torch.Size([5, 3, 400, 600])\n",
      "cuda torch.Size([5, 3, 400, 600])\n"
     ]
    }
   ],
   "source": [
    "# from model import *\n",
    "# model = Network(stage=3)\n",
    "# for i in range(5):\n",
    "for batch_idx, (in_, sem_, imgname_, semname_ ) in enumerate(train_queue):\n",
    "\n",
    "    in_ = in_.to(\"cuda\")\n",
    "    sem_ = sem_.to(\"cuda\")\n",
    "    if(batch_idx%20==0):\n",
    "        print(\"cuda\", sem_.shape)\n",
    "    # print(sem_.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7531a79e-3bc5-48be-920c-7f631db3935a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29360128, 29360128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc69c05-3278-4931-bcb1-b3ee375c58f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29360128, 29360128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved(), torch.cuda.max_memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a86c4ed4-1150-44fc-9248-bf91dcfbee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 10 15:13:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:4F:00.0 Off |                  N/A |\n",
      "| 46%   27C    P8    29W / 350W |   4416MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06eec8dd-7221-49d5-9a7a-8af47b65fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "del in_, sem_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8893c153-3dde-4c8e-aa91-402682a986f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75df9331-eacc-4755-a85a-9607bededd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 29360128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "137587cc-b1a6-47d4-adc1-c091cc64c9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29360128, 29360128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved(), torch.cuda.max_memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed395a3d-c5ba-4b4c-a9bb-8cdab13a8e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 10 11:26:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:4F:00.0 Off |                  N/A |\n",
      "| 46%   23C    P8    29W / 350W |   4416MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d67ae9-5e8a-4446-8da3-3112f56d180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99e9b0ac-0ee1-4c3a-998a-c30e164a29da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 29360128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6943272-aa43-4034-b59c-ee3ce24261d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 29360128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved(), torch.cuda.max_memory_reserved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0524ff0d-cf86-4ab4-8663-612379f7ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 10 15:14:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:4F:00.0 Off |                  N/A |\n",
      "| 46%   25C    P8    28W / 350W |   4388MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1195bbc-1fa8-44a3-a903-24292cd81226",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = []\n",
    "transform_list += [transforms.ToTensor()]\n",
    "transform = transforms.Compose(transform_list)\n",
    "\n",
    "def load_images_transform(file):\n",
    "    im = Image.open(file).convert('RGB')\n",
    "    img_norm = transform(im).numpy()\n",
    "    return img_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01468d8a-e5f0-445d-88ba-bc35379de619",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.08627451 0.11764706 0.12156863 ... 0.11372549 0.11764706 0.10980392]\n",
      "  [0.11764706 0.13333334 0.1254902  ... 0.1254902  0.10588235 0.12941177]\n",
      "  [0.1254902  0.13333334 0.13333334 ... 0.11764706 0.12156863 0.11764706]\n",
      "  ...\n",
      "  [0.00392157 0.         0.00392157 ... 0.20784314 0.2        0.21568628]\n",
      "  [0.         0.00784314 0.00392157 ... 0.20784314 0.21568628 0.22745098]\n",
      "  [0.00392157 0.         0.         ... 0.20784314 0.21960784 0.20392157]]\n",
      "\n",
      " [[0.14509805 0.1764706  0.16862746 ... 0.14901961 0.13333334 0.14117648]\n",
      "  [0.15294118 0.16862746 0.18431373 ... 0.14509805 0.14509805 0.14509805]\n",
      "  [0.18039216 0.16862746 0.18431373 ... 0.14509805 0.14509805 0.14117648]\n",
      "  ...\n",
      "  [0.01960784 0.01176471 0.01176471 ... 0.3019608  0.29411766 0.30980393]\n",
      "  [0.02745098 0.02352941 0.01176471 ... 0.30588236 0.29803923 0.3019608 ]\n",
      "  [0.01568628 0.01568628 0.01568628 ... 0.30980393 0.30588236 0.3019608 ]]\n",
      "\n",
      " [[0.15294118 0.21568628 0.21568628 ... 0.16470589 0.15686275 0.15294118]\n",
      "  [0.1882353  0.2        0.21568628 ... 0.16078432 0.15686275 0.16078432]\n",
      "  [0.20784314 0.19607843 0.21960784 ... 0.14901961 0.16078432 0.14901961]\n",
      "  ...\n",
      "  [0.04705882 0.03921569 0.05490196 ... 0.36862746 0.37254903 0.3647059 ]\n",
      "  [0.04313726 0.03921569 0.03921569 ... 0.37254903 0.37254903 0.38039216]\n",
      "  [0.05098039 0.03921569 0.03137255 ... 0.36862746 0.36862746 0.36862746]]]\n"
     ]
    }
   ],
   "source": [
    "low = load_images_transform(\"data/medium/00001.png\")\n",
    "low.shape, low.max()\n",
    "print(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dbcd153-e72e-4879-9c1e-04621256f2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 400, 600), 1.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = load_images_transform(\"../LOL/our485/high/106.png\")\n",
    "s2.shape, s2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c3f0ffd-2b7b-4f1a-92c1-e074c3218900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0035819053"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "969a87fc-c38e-4c0b-88e1-f837e233880b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 400, 600), 1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = load_images_transform(\"../LOL/our485/high_semantic/100_semantic.png\")\n",
    "s.shape, s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f108b677-d166-428a-ac3f-10adbd813d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6df002-1d9a-419e-a60f-b50de13329cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "496a052f-3d3e-4b49-8531-72607f5dab25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 448, 1119)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(transforms.ToTensor()(Image.open(\"Figs/Det_1.png\").convert('RGB'))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73d8f8d6-92da-4641-bcc5-93f962a3315e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 1119, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Image.open(\"Figs/Det_1.png\").convert('RGB')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319ec5e3-ac33-497e-a4ea-d01ec8d96a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "blocks = torch.nn.ModuleList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4d2ac9-28c9-4a3d-b4f4-a793dbe26700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2374/240228064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mto\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36madd_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             raise TypeError(\"{} is not a Module subclass\".format(\n\u001b[0m\u001b[1;32m    378\u001b[0m                 torch.typename(module)))\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "blocks.append([1,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
